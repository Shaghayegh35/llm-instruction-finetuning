{
  "base_model": "gpt2",
  "max_length": 512,
  "train_batch_size": 2,
  "eval_batch_size": 2,
  "learning_rate": 2e-05,
  "num_train_epochs": 1,
  "lora_r": 8,
  "lora_alpha": 16,
  "lora_dropout": 0.05,
  "seed": 42,
  "output_dir": "runs/ft-model"
}
